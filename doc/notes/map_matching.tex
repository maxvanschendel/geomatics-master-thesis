\subsection{Map Matching}
The process of identifying overlapping areas between partial maps is called map matching. In the case of topometric map matching, this refers to identifying which nodes represent the same rooms between two partial maps. We denote our two partial topometric maps as

\begin{equation}
    \label{eq:tmap_a}
    \topometricmap_a = (\mathcal{G}_a,\ \voxelgrid_a),\ \mathcal{G}_a=(N_a,E_a)
\end{equation}

\begin{equation}
    \label{eq:tmap_b}
    \topometricmap_b = (\mathcal{G}_b,\ \voxelgrid_b),\ \mathcal{G}_b=(N_b,E_b)
\end{equation}

The goal of map matching is to find a one-to-one mapping between the nodes of both partial maps  which corresponds to the real world and is robust to differences in coordinate system, resolution and quality between partial maps. 

\begin{equation}
    \label{eq:match_01}
    match: n_a \mapsto n_b,\ n_a \in N_a,\ n_b \in N_b \mid n_b = null
\end{equation}
\begin{equation}
    \label{eq:match_02}
    match(n_a) = 
    \begin{cases}
        n_b & \\
        null & \\
    \end{cases}
\end{equation}

To identify matches between nodes we need to be able to compute the similarity between them. To do so, we must first transform each node into a feature vector which represents both the node itself and its relationship to its neighbourhood. The feature vectors of two nodes with similar geometry and a similar neighbourhood should be close to eachother, meaning their distance in feature space is small. Conversely, the feature vectors of two dissimilar nodes should be far away from eachother. The first step of this process, encoding the node's geometry into a feature vector, is called geometric feature embedding. The second step, encoding both the geometric feature embedding of the node itself and of its neighbourhood into a new feature vector is called attributed node embedding. Figure \ref{fig:flowchart_match} shows an overview of the steps described above.

\begin{figure}[h]
    \centering
    \includegraphics*[width=0.5\textwidth]{./fig/flowchart_match.pdf}
    \caption{Diagram showing map matching methodology.}
    \label{fig:flowchart_match}
\end{figure}

In this subsection we will discuss multiple algorithms used for geometrical feature embedding and attributed node embedding. We will also discuss how we identify matches between nodes based on their feature vectors.

\subsubsection{Geometrical Feature Embedding}
Geometric feature embedding transforms a geometric object, in our case a voxel grid, into an m-dimensional feature vector \(f_{geometry}\), such that objects with similar geometry are nearby in feature space and vice versa.

\begin{equation}
    \label{eq:f}
    f_{geometry} \in \mathbb{R}^m
\end{equation}
\begin{equation}
    \label{eq:embed_geometry_01}
    embed_{geometry}: \voxelset \mapsto \mathbb{R}^m
\end{equation}
\begin{equation}
    \label{eq:embed_geometry_02}
    embed_{geometry}(n) = \prescript{n}{}{f_{geometry}}
\end{equation}

We denote the function that embeds a set of voxels into a feature vector as in equation \ref{eq:embed_geometry_01}. We implement this function using two different approaches, which we discuss below.

\subsubsection{Spectral Features}
Our first approach to geometric feature embedding uses spectral shape analysis. This approach uses the eigenvalues of the graph laplacian, the graph in our case being the neighbourhood graph of a voxel grid, to find a low-dimensional global descriptor of a shape. 

The laplacian matrix of a graph can be found by subtracting its adjacency matrix from its degree matrix, as shown in equation \ref{eq:laplacian_matrix}.

\begin{equation}
    \label{eq:laplacian_matrix}
L = D - A
\end{equation}

After computing the Laplacian matrix we then find its \(n\) smallest 
non-zero eigenvalues which gives us a global descriptor of the voxel grid. 


\subsubsection{Deep Learning}
Our second approach to geometric feature embedding uses deep learning. Specifically, we use the LPDNet neural network architecture. This architecture is used for place recognition, it does so by learning embeddings, typically 2048 or 4096-dimensional, of point clouds that are theoretically independent of transformation, perspective and completeness. It does so by computing a feature embedding for every point in the point cloud and aggregrating them into a global feature embedding. The LPDNet model we use is trained on outdoor maps which have different characteristics from indoor maps. However, the authors of LPDNet claim that a model trained on outdoor data can also effectively be used for indoor data. Figure \ref{fig:lpdnet_architecture} shows the network architecture of LPDNet.

% network architecture w/figure
% training data / transfer learning

\begin{figure}[h]
    \centering
    \includegraphics*[width=\textwidth]{./fig/network_architecture.png}
    \caption{Diagram showing LPDNet network architecture.}
    \label{fig:lpdnet_architecture}
\end{figure}

\subsubsection{Contextual Embedding}


\subsubsection{Map Matching}
The above steps are applied to both partial maps. This gives us two sets of feature vectors \(\mathcal{E}_A,\ \mathcal{E}_B\) representing the embedding of the nodes of both topometric maps.

To identify the most likely overlapping rooms between the partial maps we find the one-to-one mapping between the elements of \(\mathcal{E}_A\) and \(\mathcal{E}_B\) that maximizes the similarity (or minimizes the distances) between the chosen pairs. This is an example of the unbalanced assignment problem, which consists of finding a matching in a weighted bipartite graph that minimizes the sum of its edge weights. It is unbalanced because there may be more nodes in one part of the bipartite graph than the other, which means it is not possible to assign every node in one part to a node in the other. 

To construct the weighted bipartite graph we first find the Cartesian product of the feature vectors 

\begin{equation}
    \label{eq:E_ab}
    \mathcal{E}_{AB} = \mathcal{E}_A \times \mathcal{E}_B = \{(a,b) \mid a \in \mathcal{E}_A,\ b \in \mathcal{E}_B\}
\end{equation}

We then compute the Euclidean distance in feature space between every pair of nodes in \(\mathcal{E}_{AB}\), creating the cost matrix that represents the weighted bipartite graph 

\begin{equation}
    \label{eq:C}
    \mathbf{C} \in \mathbb{R}^{|V_a| \times |V_b|}
\end{equation}
\begin{equation}
    \label{eq:C}
    \mathbf{C}_{ij} = ||a - b||,\ (a,\ b) \in \mathcal{E}_{AB},\ a = \mathcal{E}_{A,\ i},\ b = \mathcal{E}_{B,\ j},\ \mathbf{C}_{ij} \in \mathbb{R}^+
\end{equation}

We can then find unbalanced assignment using various approaches, in our case the Jonker-Volgenant algorithm [CITE]. We denote the resulting matching between the nodes of both partial maps and their distance in feature space as a set of triples 

\begin{equation}
    \label{eq:M}
    \mathbf{M} = \{(i,\ j,\ d) \mid  |V_a| \geq i,\ |V_b|\geq j,\ d = \mathbf{C}_{ij}\}
\end{equation}

\subsubsection{Hypothesis growing}

In practice it is unlikely that every match in \(\mathbf{M}\) is correct. However, we can use them as seeds to generate hypotheses similar to the approach described in Huang \citep{huang_topological_2005}. Starting at each match 

\begin{equation}
    \label{eq:init_match}
    (v_i,\ v_j) = (V_{A,\ i},\ V_{B,\ j}),\ (i,\ j) \in \mathbf{M}
\end{equation}


we get the neighbourhood of both nodes. We then construct a new cost matrix from the Euclidean distance between the embeddings of both neighbourhoods, again creating a weighted bipartite graph for which we can solve the assignment problem. By doing this we identify which neighbours of the nodes in the match are most likely to also match. We recursively apply this step to the matching neighbours to grow our initial matches into hypotheses. To decrease the risk of incorrectly identifying neighbourhood matches we constrain hypothesis growing in two ways. First, the cost of two potential matches must be below a given threshold \(c_{max}\). Second, a newly identified match may not bring the existing matching too much out of alignment. To check this, we perform coarse registration between the centroids of the geometry of the identified matches at every step of the hypothesis growing using least squares adjustment. If the error increases between steps, and the increase is too large such that \(\triangle e \geq \triangle e_{max} \), then the matching is rejected. By adjusting the values of \(c_{max}\) and \(\triangle e_{max}\) more or less uncertainty is allowed when growing hypotheses.
